{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# Written by Greg Ver Steeg\n",
    "# See readme.pdf for documentation\n",
    "# Or go to http://www.isi.edu/~gregv/npeet.html\n",
    "\n",
    "import scipy.spatial as ss\n",
    "from scipy.special import digamma\n",
    "from math import log\n",
    "import numpy.random as nr\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# CONTINUOUS ESTIMATORS\n",
    "\n",
    "def entropy(x, k=3, base=2):\n",
    "    \"\"\" The classic K-L k-nearest neighbor continuous entropy estimator\n",
    "        x should be a list of vectors, e.g. x = [[1.3], [3.7], [5.1], [2.4]]\n",
    "        if x is a one-dimensional scalar and we have four samples\n",
    "    \"\"\"\n",
    "    assert k <= len(x) - 1, \"Set k smaller than num. samples - 1\"\n",
    "    x = to_np_array(x)\n",
    "    d = len(x[0])\n",
    "    N = len(x)\n",
    "    intens = 1e-10  # small noise to break degeneracy, see doc.\n",
    "    x = [list(p + intens * nr.rand(len(x[0]))) for p in x]\n",
    "    tree = ss.cKDTree(x)\n",
    "    nn = [tree.query(point, k + 1, p=float('inf'))[0][k] for point in x]\n",
    "    const = digamma(N) - digamma(k) + d * log(2)\n",
    "    return (const + d * np.mean(np.log(nn))) / log(base)\n",
    "\n",
    "def centropy(x, y, k=3, base=2):\n",
    "    \"\"\" The classic K-L k-nearest neighbor continuous entropy estimator for the\n",
    "        entropy of X conditioned on Y.\n",
    "    \"\"\"\n",
    "    x, y = to_np_array(x, y)\n",
    "    hxy = entropy([xi + yi for (xi, yi) in zip(x, y)], k, base)\n",
    "    hy = entropy(y, k, base)\n",
    "    return hxy - hy\n",
    "\n",
    "def column(xs, i):\n",
    "    return [[x[i]] for x in xs]\n",
    "\n",
    "def tc(xs, k=3, base=2):\n",
    "    xis = [entropy(column(xs, i), k, base) for i in range(0, len(xs[0]))]\n",
    "    return np.sum(xis) - entropy(xs, k, base)\n",
    "\n",
    "def ctc(xs, y, k=3, base=2):\n",
    "    xis = [centropy(column(xs, i), y, k, base) for i in range(0, len(xs[0]))]\n",
    "    return np.sum(xis) - centropy(xs, y, k, base)\n",
    "\n",
    "def corex(xs, ys, k=3, base=2):\n",
    "    cxis = [mi(column(xs, i), ys, k, base) for i in range(0, len(xs[0]))]\n",
    "    return np.sum(cxis) - mi(xs, ys, k, base)\n",
    "\n",
    "def mi(x, y, k=3, base=2):\n",
    "    \"\"\" Mutual information of x and y\n",
    "        x, y should be a list of vectors, e.g. x = [[1.3], [3.7], [5.1], [2.4]]\n",
    "        if x is a one-dimensional scalar and we have four samples\n",
    "    \"\"\"\n",
    "    assert len(x) == len(y), \"Lists should have same length\"\n",
    "    assert k <= len(x) - 1, \"Set k smaller than num. samples - 1\"\n",
    "    x, y = flatten(*to_np_array(x, y))\n",
    "    intens = 1e-10  # small noise to break degeneracy, see doc.\n",
    "    x = [list(p + intens * nr.rand(len(x[0]))) for p in x]\n",
    "    y = [list(p + intens * nr.rand(len(y[0]))) for p in y]\n",
    "    points = zip2(x, y)\n",
    "    # Find nearest neighbors in joint space, p=inf means max-norm\n",
    "    tree = ss.cKDTree(points)\n",
    "    dvec = [tree.query(point, k + 1, p=float('inf'))[0][k] for point in points]\n",
    "    a, b, c, d = avgdigamma(x, dvec), avgdigamma(y, dvec), digamma(k), digamma(len(x))\n",
    "    return (-a - b + c + d) / log(base)\n",
    "\n",
    "\n",
    "def cmi(x, y, z, k=3, base=2):\n",
    "    \"\"\" Mutual information of x and y, conditioned on z\n",
    "        x, y, z should be a list of vectors, e.g. x = [[1.3], [3.7], [5.1], [2.4]]\n",
    "        if x is a one-dimensional scalar and we have four samples\n",
    "    \"\"\"\n",
    "    assert len(x) == len(y), \"Lists should have same length\"\n",
    "    assert k <= len(x) - 1, \"Set k smaller than num. samples - 1\"\n",
    "    x, y, z = flatten(*to_np_array(x, y, z))\n",
    "    intens = 1e-10  # small noise to break degeneracy, see doc.\n",
    "    x = [list(p + intens * nr.rand(len(x[0]))) for p in x]\n",
    "    y = [list(p + intens * nr.rand(len(y[0]))) for p in y]\n",
    "    z = [list(p + intens * nr.rand(len(z[0]))) for p in z]\n",
    "    points = zip2(x, y, z)\n",
    "    # Find nearest neighbors in joint space, p=inf means max-norm\n",
    "    tree = ss.cKDTree(points)\n",
    "    dvec = [tree.query(point, k + 1, p=float('inf'))[0][k] for point in points]\n",
    "    a, b, c, d = avgdigamma(zip2(x, z), dvec), avgdigamma(zip2(y, z), dvec), avgdigamma(z, dvec), digamma(k)\n",
    "    return (-a - b + c + d) / log(base)\n",
    "\n",
    "\n",
    "def kldiv(x, xp, k=3, base=2):\n",
    "    \"\"\" KL Divergence between p and q for x~p(x), xp~q(x)\n",
    "        x, xp should be a list of vectors, e.g. x = [[1.3], [3.7], [5.1], [2.4]]\n",
    "        if x is a one-dimensional scalar and we have four samples\n",
    "    \"\"\"\n",
    "    assert k <= len(x) - 1, \"Set k smaller than num. samples - 1\"\n",
    "    assert k <= len(xp) - 1, \"Set k smaller than num. samples - 1\"\n",
    "    assert len(x[0]) == len(xp[0]), \"Two distributions must have same dim.\"\n",
    "    x, xp = to_np_array(x, xp)\n",
    "    d = len(x[0])\n",
    "    n = len(x)\n",
    "    m = len(xp)\n",
    "    const = log(m) - log(n - 1)\n",
    "    tree = ss.cKDTree(x)\n",
    "    treep = ss.cKDTree(xp)\n",
    "    nn = [tree.query(point, k + 1, p=float('inf'))[0][k] for point in x]\n",
    "    nnp = [treep.query(point, k, p=float('inf'))[0][k - 1] for point in x]\n",
    "    return (const + d * np.mean(np.log(nnp)) - d * np.mean(np.log(nn))) / log(base)\n",
    "\n",
    "\n",
    "# DISCRETE ESTIMATORS\n",
    "def entropyd(sx, base=2):\n",
    "    \"\"\" Discrete entropy estimator\n",
    "        Given a list of samples which can be any hashable object\n",
    "    \"\"\"\n",
    "    sx = to_np_array(sx)\n",
    "    return entropyfromprobs(hist(sx), base=base)\n",
    "\n",
    "\n",
    "def midd(x, y, base=2):\n",
    "    \"\"\" Discrete mutual information estimator\n",
    "        Given a list of samples which can be any hashable object\n",
    "    \"\"\"\n",
    "    x, y = flatten(*to_np_array(x, y))\n",
    "    return -entropyd(zip(x, y), base) + entropyd(x, base) + entropyd(y, base)\n",
    "\n",
    "def cmidd(x, y, z):\n",
    "    \"\"\" Discrete mutual information estimator\n",
    "        Given a list of samples which can be any hashable object\n",
    "    \"\"\"\n",
    "    x, y, z = flatten(*to_np_array(x, y, z))\n",
    "    return entropyd(zip(y, z)) + entropyd(zip(x, z)) - entropyd(zip(x, y, z)) - entropyd(z)\n",
    "\n",
    "def centropyd(x, y, base=2):\n",
    "    \"\"\" The classic K-L k-nearest neighbor continuous entropy estimator for the\n",
    "        entropy of X conditioned on Y.\n",
    "    \"\"\"\n",
    "    x, y = flatten(*to_np_array(x, y))\n",
    "    return entropyd(zip(x, y), base) - entropyd(y, base)\n",
    "\n",
    "def tcd(xs, base=2):\n",
    "    xis = [entropyd(column(xs, i), base) for i in range(0, len(xs[0]))]\n",
    "    hx = entropyd(xs, base)\n",
    "    return np.sum(xis) - hx\n",
    "\n",
    "def ctcd(xs, y, base=2):\n",
    "    xis = [centropyd(column(xs, i), y, base) for i in range(0, len(xs[0]))]\n",
    "    return np.sum(xis) - centropyd(xs, y, base)\n",
    "\n",
    "def corexd(xs, ys, base=2):\n",
    "    cxis = [midd(column(xs, i), ys, base) for i in range(0, len(xs[0]))]\n",
    "    return np.sum(cxis) - midd(xs, ys, base)\n",
    "\n",
    "def hist(sx):\n",
    "    sx = discretize(sx)\n",
    "    # Histogram from list of samples\n",
    "    d = dict()\n",
    "    for s in sx:\n",
    "        if type(s) == list:\n",
    "            s = tuple(s)\n",
    "        d[s] = d.get(s, 0) + 1\n",
    "    return map(lambda z: float(z) / len(sx), d.values())\n",
    "\n",
    "\n",
    "def entropyfromprobs(probs, base=2):\n",
    "    # Turn a normalized list of probabilities of discrete outcomes into entropy (base 2)\n",
    "    return -sum(map(elog, probs)) / log(base)\n",
    "\n",
    "\n",
    "def elog(x):\n",
    "    # for entropy, 0 log 0 = 0. but we get an error for putting log 0\n",
    "    if x <= 0. or x >= 1.:\n",
    "        return 0\n",
    "    else:\n",
    "        return x * log(x)\n",
    "\n",
    "\n",
    "# MIXED ESTIMATORS\n",
    "def micd(x, y, k=3, base=2, warning=True):\n",
    "    \"\"\" If x is continuous and y is discrete, compute mutual information\n",
    "    \"\"\"\n",
    "    x, y = flatten(*to_np_array(x, y))\n",
    "    overallentropy = entropy(x, k, base)\n",
    "\n",
    "    n = len(y)\n",
    "    word_dict = dict()\n",
    "    for i in range(len(y)):\n",
    "        if type(y[i]) == list:\n",
    "            y[i] = tuple(y[i])\n",
    "    for sample in y:\n",
    "        word_dict[sample] = word_dict.get(sample, 0) + 1. / n\n",
    "    yvals = list(set(word_dict.keys()))\n",
    "\n",
    "    mi = overallentropy\n",
    "    for yval in yvals:\n",
    "        xgiveny = [x[i] for i in range(n) if y[i] == yval]\n",
    "        if k <= len(xgiveny) - 1:\n",
    "            mi -= word_dict[yval] * entropy(xgiveny, k, base)\n",
    "        else:\n",
    "            if warning:\n",
    "                print(\"Warning, after conditioning, on y=\", yval, \" insufficient data. Assuming maximal entropy in this case.\")\n",
    "            mi -= word_dict[yval] * overallentropy\n",
    "    return np.abs(mi)  # units already applied\n",
    "\n",
    "def midc(x, y, k=3, base=2, warning=True):\n",
    "    x, y = flatten(*to_np_array(x, y))\n",
    "    return micd(y, x, k, base, warning)\n",
    "\n",
    "def centropydc(x, y, k=3, base=2, warning=True):\n",
    "    x, y = flatten(*to_np_array(x, y))\n",
    "    return entropyd(x, base) - midc(x, y, k, base, warning)\n",
    "\n",
    "def centropycd(x, y, k=3, base=2, warning=True):\n",
    "    x, y = flatten(*to_np_array(x, y))\n",
    "    return entropy(x, k, base) - micd(x, y, k, base, warning)\n",
    "\n",
    "def ctcdc(xs, y, k=3, base=2, warning=True):\n",
    "    xis = [centropydc(column(xs, i), y, k, base, warning) for i in range(0, len(xs[0]))]\n",
    "    return np.sum(xis) - centropydc(xs, y, k, base, warning)\n",
    "\n",
    "def ctccd(xs, y, k=3, base=2, warning=True):\n",
    "    xis = [centropycd(column(xs, i), y, k, base, warning) for i in range(0, len(xs[0]))]\n",
    "    return np.sum(xis) - centropycd(xs, y, k, base, warning)\n",
    "\n",
    "def corexcd(xs, ys, k=3, base=2, warning=True):\n",
    "    cxis = [micd(column(xs, i), ys, k, base, warning) for i in range(0, len(xs[0]))]\n",
    "    return np.sum(cxis) - micd(xs, ys, k, base, warning)\n",
    "\n",
    "def corexdc(xs, ys, k=3, base=2, warning=True):\n",
    "    #cxis = [midc(column(xs, i), ys, k, base, warning) for i in range(0, len(xs[0]))]\n",
    "    #joint = midc(xs, ys, k, base, warning)\n",
    "    #return np.sum(cxis) - joint\n",
    "    return tcd(xs, base) - ctcdc(xs, ys, k, base, warning)\n",
    "\n",
    "# UTILITY FUNCTIONS\n",
    "def vectorize(scalarlist):\n",
    "    \"\"\" Turn a list of scalars into a list of one-d vectors\n",
    "    \"\"\"\n",
    "    return [[x] for x in scalarlist]\n",
    "\n",
    "\n",
    "def shuffle_test(measure, x, y, z=False, ns=200, ci=0.95, **kwargs):\n",
    "    \"\"\" Shuffle test\n",
    "        Repeatedly shuffle the x-values and then estimate measure(x, y, [z]).\n",
    "        Returns the mean and conf. interval ('ci=0.95' default) over 'ns' runs.\n",
    "        'measure' could me mi, cmi, e.g. Keyword arguments can be passed.\n",
    "        Mutual information and CMI should have a mean near zero.\n",
    "    \"\"\"\n",
    "    xp = x[:]  # A copy that we can shuffle\n",
    "    outputs = []\n",
    "    for i in range(ns):\n",
    "        random.shuffle(xp)\n",
    "        if z:\n",
    "            outputs.append(measure(xp, y, z, **kwargs))\n",
    "        else:\n",
    "            outputs.append(measure(xp, y, **kwargs))\n",
    "    outputs.sort()\n",
    "    return np.mean(outputs), (outputs[int((1. - ci) / 2 * ns)], outputs[int((1. + ci) / 2 * ns)])\n",
    "\n",
    "\n",
    "# INTERNAL FUNCTIONS\n",
    "\n",
    "def avgdigamma(points, dvec):\n",
    "    # This part finds number of neighbors in some radius in the marginal space\n",
    "    # returns expectation value of <psi(nx)>\n",
    "    N = len(points)\n",
    "    tree = ss.cKDTree(points)\n",
    "    avg = 0.\n",
    "    for i in range(N):\n",
    "        dist = dvec[i]\n",
    "        # subtlety, we don't include the boundary point,\n",
    "        # but we are implicitly adding 1 to kraskov def bc center point is included\n",
    "        num_points = len(tree.query_ball_point(points[i], dist - 1e-15, p=float('inf')))\n",
    "        avg += digamma(num_points) / N\n",
    "    return avg\n",
    "\n",
    "\n",
    "def zip2(*args):\n",
    "    # zip2(x, y) takes the lists of vectors and makes it a list of vectors in a joint space\n",
    "    # E.g. zip2([[1], [2], [3]], [[4], [5], [6]]) = [[1, 4], [2, 5], [3, 6]]\n",
    "    return [sum(sublist, []) for sublist in zip(*args)]\n",
    "\n",
    "def discretize(xs):\n",
    "    def discretize_one(x):\n",
    "        if len(x) > 1:\n",
    "            if isinstance(x[0], list):\n",
    "                return tuple([tuple(element) for element in x])\n",
    "            else:\n",
    "                return tuple(x)\n",
    "        else:\n",
    "            if isinstance(x[0], list):\n",
    "                return tuple(x[0])\n",
    "            else:\n",
    "                return x[0]\n",
    "    # discretize(xs) takes a list of vectors and makes it a list of tuples or scalars\n",
    "    return [discretize_one(x) for x in xs]\n",
    "\n",
    "\n",
    "\n",
    "# Functions for adaptations with PyTorch:\n",
    "def to_np_array(*arrays):\n",
    "    \"\"\"Transform torch tensors/Variables into numpy arrays\"\"\"\n",
    "    array_list = []\n",
    "    for array in arrays:\n",
    "        if isinstance(array, Variable):\n",
    "            if array.is_cuda:\n",
    "                array = array.cpu()\n",
    "            array = array.data\n",
    "        if isinstance(array, torch.FloatTensor) or isinstance(array, torch.LongTensor) or isinstance(array, torch.ByteTensor) or \\\n",
    "           isinstance(array, torch.cuda.FloatTensor) or isinstance(array, torch.cuda.LongTensor) or isinstance(array, torch.cuda.ByteTensor):\n",
    "            if array.is_cuda:\n",
    "                array = array.cpu()\n",
    "            array = array.numpy()\n",
    "        array_list.append(array)\n",
    "    if len(array_list) == 1:\n",
    "        array_list = array_list[0]\n",
    "    return array_list\n",
    "\n",
    "\n",
    "def flatten(*tensors):\n",
    "    \"\"\"Flatten the tensor except the first dimension\"\"\"\n",
    "    new_tensors = []\n",
    "    for tensor in tensors:\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            new_tensor = tensor.contiguous().view(tensor.shape[0], -1)\n",
    "        elif isinstance(tensor, np.ndarray):\n",
    "            new_tensor = tensor.reshape(tensor.shape[0], -1)\n",
    "        elif isinstance(tensor, list):\n",
    "            tensor = np.array(tensor)\n",
    "            new_tensor = tensor.reshape(tensor.shape[0], -1).tolist()\n",
    "        else:\n",
    "            print(tensor)\n",
    "            raise Exception(\"tensors must be either torch.Tensor or np.ndarray!\")\n",
    "        new_tensors.append(new_tensor)\n",
    "    if len(new_tensors) == 1:\n",
    "        new_tensors = new_tensors[0]\n",
    "    return new_tensors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
